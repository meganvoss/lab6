[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "library(ggthemes)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(dplyr)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#zero_q_freq-frequency-of-days-with-q-0-mmday-its-reported-in-a-percentage",
    "href": "lab6.html#zero_q_freq-frequency-of-days-with-q-0-mmday-its-reported-in-a-percentage",
    "title": "Lab 6",
    "section": "zero_q_freq: frequency of days with Q = 0 mm/day (it’s reported in a percentage)",
    "text": "zero_q_freq: frequency of days with Q = 0 mm/day (it’s reported in a percentage)"
  },
  {
    "objectID": "lab6.html#correct-version",
    "href": "lab6.html#correct-version",
    "title": "Lab 6",
    "section": "correct version",
    "text": "correct version\n\nlibrary(recipes)\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nlibrary(yardstick)\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nlibrary(ggplot2)\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab6.html#i-just-realized-i-dont-think-we-need-to-put-in-all-the-code-given",
    "href": "lab6.html#i-just-realized-i-dont-think-we-need-to-put-in-all-the-code-given",
    "title": "Lab 6",
    "section": "I just realized I don’t think we need to put in all the code given :)",
    "text": "I just realized I don’t think we need to put in all the code given :)"
  },
  {
    "objectID": "lab6.html#linear-regression-seems-like-it-would-be-the-best-for-this",
    "href": "lab6.html#linear-regression-seems-like-it-would-be-the-best-for-this",
    "title": "Lab 6",
    "section": "Linear regression seems like it would be the best for this!",
    "text": "Linear regression seems like it would be the best for this!"
  },
  {
    "objectID": "lab6.html#build-your-own",
    "href": "lab6.html#build-your-own",
    "title": "Lab 6",
    "section": "Build your own",
    "text": "Build your own"
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6",
    "section": "data splitting",
    "text": "data splitting\n\nset.seed(123)  \n\ndata_split &lt;- initial_split(camels, prop = 0.75, strata = q_mean)\ncamels_train &lt;- training(data_split)\ncamels_test &lt;- testing(data_split)\n\ncv_folds &lt;- vfold_cv(camels_train, v = 10, strata = q_mean)\n\ndim(camels_train)  \n\n[1] 503  59\n\ndim(camels_test)   \n\n[1] 168  59"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6",
    "section": "recipe",
    "text": "recipe\n\nlibrary(recipes)\n\nrec &lt;- recipe(q_mean ~ p_mean + aridity + elev_mean + slope_mean + \n                 geol_permeability + baseflow_index + runoff_ratio, \n              data = camels_train) %&gt;%\n  step_log(q_mean, base = 10) %&gt;%  \n  step_normalize(all_numeric_predictors()) %&gt;%  \n  step_impute_median(all_numeric_predictors()) %&gt;%  \n  step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%  \n  step_nzv(all_predictors())  \n\nrec &lt;- prep(rec, training = camels_train)\n\ntrain_prepped &lt;- bake(rec, new_data = camels_train)\nhead(train_prepped)\n\n# A tibble: 6 × 8\n  p_mean aridity elev_mean slope_mean geol_permeability baseflow_index\n   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1  0.251 -0.0103    -0.921     -0.939             2.06          0.380 \n2  0.292 -0.169     -0.911     -0.941             1.77          0.569 \n3  0.303 -0.0751    -0.914     -0.931             1.38          0.482 \n4  0.434 -0.107     -0.925     -0.955             0.765         0.0394\n5 -1.37   0.810     -0.326     -0.880             0.302        -0.246 \n6 -1.34   0.696     -0.342     -0.886             0.245         0.0675\n# ℹ 2 more variables: runoff_ratio &lt;dbl&gt;, q_mean &lt;dbl&gt;"
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6",
    "section": "define 3 models",
    "text": "define 3 models\n\nlibrary(parsnip)\n\nrf_model &lt;- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_model &lt;- boost_tree(trees = tune(), tree_depth = tune(), learn_rate = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(baguette)\n\nnn_model &lt;- bag_mlp(hidden_units = tune(), penalty = tune()) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6",
    "section": "workflow set",
    "text": "workflow set\n\nrec &lt;- recipe(q_mean ~ p_mean + aridity + elev_mean + slope_mean + \n                 geol_permeability + baseflow_index + runoff_ratio, \n              data = camels_train) %&gt;%\n  step_log(q_mean, base = 10) %&gt;%  \n  step_normalize(all_numeric_predictors()) %&gt;%  \n  step_impute_median(all_numeric_predictors()) %&gt;%  \n  step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%  \n  step_nzv(all_predictors())  \n\n\n# Random Forest \nrf_model &lt;- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# XGBoost model \nxgb_model &lt;- boost_tree(trees = tune(), tree_depth = tune(), learn_rate = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Neural Network \nnn_model &lt;- bag_mlp(hidden_units = tune(), penalty = tune()) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nmodel_set &lt;- workflow_set(\n  preproc = list(\"recipe\" = rec),\n  models = list(\n    \"Random Forest\" = rf_model, \n    \"XGBoost\" = xgb_model, \n    \"Neural Network\" = nn_model\n  )\n)\n\n\ncamels_train &lt;- camels_train %&gt;%\n  filter(!is.na(q_mean))\n\n\nmodel_set &lt;- workflow_set(\n  preproc = list(\"recipe\" = rec),\n  models = list(\n    \"Random Forest\" = rf_model, \n    \"XGBoost\" = xgb_model, \n    \"Neural Network\" = nn_model\n  )\n)\n\nmodel_fit &lt;- model_set %&gt;%\n  workflow_map(\n    \"tune_grid\",  \n    resamples = cv_folds,\n    metrics = metric_set(rmse, rsq, mae),\n    grid = 10,  \n    verbose = TRUE\n  )\n\ni 1 of 3 tuning:     recipe_Random Forest\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n→ A | error:   Error: Missing data in dependent variable.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x31\n\n\nThere were issues with some computations   A: x90\n\n\n\n\n\n✔ 1 of 3 tuning:     recipe_Random Forest (3s)\n\n\ni 2 of 3 tuning:     recipe_XGBoost\n\n\n→ A | error:   [14:35:16] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010b315c7c dmlc::LogMessageFatal::~LogMessageFatal() + 124\n                 [bt] (1) 2   xgboost.so                          0x000000010b3ad460 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 3616\n                 [bt] (2) 3   xgboost.so                          0x000000010b3ae6e8 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 168\n                 [bt] (3) 4   xgboost.so                          0x000000010b4cc184 XGDMatrixSetFloatInfo + 132\n                 [bt] (4) 5   xgboost.so                          0x000000010b310eec XGDMatrixSetInfo_R + 620\n                 [bt] (5) 6   libR.dylib                          0x00000001032409b4 R_doDotCall + 1588\n                 [bt] (6) 7   libR.dylib                          0x000000010329ccfc bcEval_loop + 128060\n                 [bt] (7) \n\n\n→ B | error:   [14:35:19] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010b315c7c dmlc::LogMessageFatal::~LogMessageFatal() + 124\n                 [bt] (1) 2   xgboost.so                          0x000000010b3ad460 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 3616\n                 [bt] (2) 3   xgboost.so                          0x000000010b3ae6e8 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 168\n                 [bt] (3) 4   xgboost.so                          0x000000010b4cc184 XGDMatrixSetFloatInfo + 132\n                 [bt] (4) 5   xgboost.so                          0x000000010b310eec XGDMatrixSetInfo_R + 620\n                 [bt] (5) 6   libR.dylib                          0x00000001032409b4 R_doDotCall + 1588\n                 [bt] (6) 7   libR.dylib                          0x000000010329ccfc bcEval_loop + 128060\n                 [bt] (7) \n\n\nThere were issues with some computations   A: x30   B: x1\n\n\n→ C | error:   [14:35:20] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010b315c7c dmlc::LogMessageFatal::~LogMessageFatal() + 124\n                 [bt] (1) 2   xgboost.so                          0x000000010b3ad460 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 3616\n                 [bt] (2) 3   xgboost.so                          0x000000010b3ae6e8 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 168\n                 [bt] (3) 4   xgboost.so                          0x000000010b4cc184 XGDMatrixSetFloatInfo + 132\n                 [bt] (4) 5   xgboost.so                          0x000000010b310eec XGDMatrixSetInfo_R + 620\n                 [bt] (5) 6   libR.dylib                          0x00000001032409b4 R_doDotCall + 1588\n                 [bt] (6) 7   libR.dylib                          0x000000010329ccfc bcEval_loop + 128060\n                 [bt] (7) \n\n\nThere were issues with some computations   A: x30   B: x1\nThere were issues with some computations   A: x30   B: x20   C: x32\nThere were issues with some computations   A: x30   B: x20   C: x40\n\n✔ 2 of 3 tuning:     recipe_XGBoost (4.8s)\ni 3 of 3 tuning:     recipe_Neural Network\n✔ 3 of 3 tuning:     recipe_Neural Network (22.6s)\n\n\n\ncollect_metrics(model_fit)\n\n# A tibble: 90 × 9\n   wflow_id        .config preproc model .metric .estimator   mean     n std_err\n   &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 recipe_Random … Prepro… recipe  rand… mae     standard   0.0841     1      NA\n 2 recipe_Random … Prepro… recipe  rand… rmse    standard   0.141      1      NA\n 3 recipe_Random … Prepro… recipe  rand… rsq     standard   0.945      1      NA\n 4 recipe_Random … Prepro… recipe  rand… mae     standard   0.102      1      NA\n 5 recipe_Random … Prepro… recipe  rand… rmse    standard   0.164      1      NA\n 6 recipe_Random … Prepro… recipe  rand… rsq     standard   0.928      1      NA\n 7 recipe_Random … Prepro… recipe  rand… mae     standard   0.0445     1      NA\n 8 recipe_Random … Prepro… recipe  rand… rmse    standard   0.0832     1      NA\n 9 recipe_Random … Prepro… recipe  rand… rsq     standard   0.979      1      NA\n10 recipe_Random … Prepro… recipe  rand… mae     standard   0.0585     1      NA\n# ℹ 80 more rows\n\nautoplot(model_fit)\n\n\n\n\n\n\n\nrank_results(model_fit)\n\n# A tibble: 90 × 9\n   wflow_id       .config .metric    mean std_err     n preprocessor model  rank\n   &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 recipe_Neural… Prepro… mae     0.00733 8.26e-4    10 recipe       bag_…     1\n 2 recipe_Neural… Prepro… rmse    0.0198  4.57e-3    10 recipe       bag_…     1\n 3 recipe_Neural… Prepro… rsq     0.998   6.53e-4    10 recipe       bag_…     1\n 4 recipe_Neural… Prepro… mae     0.00954 8.39e-4    10 recipe       bag_…     2\n 5 recipe_Neural… Prepro… rmse    0.0214  5.21e-3    10 recipe       bag_…     2\n 6 recipe_Neural… Prepro… rsq     0.998   8.07e-4    10 recipe       bag_…     2\n 7 recipe_Neural… Prepro… mae     0.00920 8.30e-4    10 recipe       bag_…     3\n 8 recipe_Neural… Prepro… rmse    0.0216  4.03e-3    10 recipe       bag_…     3\n 9 recipe_Neural… Prepro… rsq     0.998   5.53e-4    10 recipe       bag_…     3\n10 recipe_Neural… Prepro… mae     0.00913 1.25e-3    10 recipe       bag_…     4\n# ℹ 80 more rows"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6",
    "section": "Evaluation",
    "text": "Evaluation\n\nlibrary(tune)\n\nranked &lt;- rank_results(model_fit)\nprint(ranked)\n\n# A tibble: 90 × 9\n   wflow_id       .config .metric    mean std_err     n preprocessor model  rank\n   &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 recipe_Neural… Prepro… mae     0.00733 8.26e-4    10 recipe       bag_…     1\n 2 recipe_Neural… Prepro… rmse    0.0198  4.57e-3    10 recipe       bag_…     1\n 3 recipe_Neural… Prepro… rsq     0.998   6.53e-4    10 recipe       bag_…     1\n 4 recipe_Neural… Prepro… mae     0.00954 8.39e-4    10 recipe       bag_…     2\n 5 recipe_Neural… Prepro… rmse    0.0214  5.21e-3    10 recipe       bag_…     2\n 6 recipe_Neural… Prepro… rsq     0.998   8.07e-4    10 recipe       bag_…     2\n 7 recipe_Neural… Prepro… mae     0.00920 8.30e-4    10 recipe       bag_…     3\n 8 recipe_Neural… Prepro… rmse    0.0216  4.03e-3    10 recipe       bag_…     3\n 9 recipe_Neural… Prepro… rsq     0.998   5.53e-4    10 recipe       bag_…     3\n10 recipe_Neural… Prepro… mae     0.00913 1.25e-3    10 recipe       bag_…     4\n# ℹ 80 more rows\n\n\n\nautoplot(model_fit)"
  },
  {
    "objectID": "lab6.html#i-think-xgboost-probably-is-the-bestmost-accurate-one.-it-has-the-highest-r-squared-value-so-least-amount-of-variability.",
    "href": "lab6.html#i-think-xgboost-probably-is-the-bestmost-accurate-one.-it-has-the-highest-r-squared-value-so-least-amount-of-variability.",
    "title": "Lab 6",
    "section": "I think XGBoost probably is the best/most accurate one. It has the highest R squared value so least amount of variability.",
    "text": "I think XGBoost probably is the best/most accurate one. It has the highest R squared value so least amount of variability."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6",
    "section": "Extract and Evaluate",
    "text": "Extract and Evaluate\n\n# 1. Create recipe\nrec &lt;- recipe(q_mean ~ p_mean + aridity + elev_mean + slope_mean + \n                geol_permeability + baseflow_index + runoff_ratio,\n              data = camels_train) %&gt;%\n  step_log(q_mean, base = 10) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%\n  step_nzv(all_predictors())\n\n\n# 2. Get best XGBoost model from tuning results\nbest_xgb &lt;- model_fit %&gt;%\n  extract_workflow_set_result(\"recipe_XGBoost\") %&gt;%\n  select_best(metric = \"rsq\")\n\nfinal_xgb &lt;- finalize_model(xgb_model, best_xgb)\n\n\n# 3. Build and fit the workflow with the final model and recipe\nfinal_xgb_wf &lt;- workflow() %&gt;%\n  add_model(final_xgb) %&gt;%\n  add_recipe(rec)\n\nfinal_xgb_fit &lt;- final_xgb_wf %&gt;% fit(data = camels_train)\n\n\n# 4. Clean test set\ncamels_test_clean &lt;- camels_test %&gt;%\n  filter(!is.na(q_mean), q_mean &gt; 0)\n\n\nxgb_fit_object &lt;- extract_fit_parsnip(final_xgb_fit)\n\n\nrec_prepped &lt;- prep(rec, training = camels_train)\n\ntest_baked &lt;- bake(rec_prepped, new_data = camels_test_clean)\n\n\ntest_preds &lt;- predict(xgb_fit_object, new_data = test_baked)\n\n\nxgb_preds &lt;- bind_cols(\n  test_baked,\n  test_preds,\n  q_mean_obs = camels_test_clean$q_mean\n) %&gt;%\n  mutate(q_mean_pred = 10^.pred)\n\n\nggplot(xgb_preds, aes(x = q_mean_pred, y = q_mean_obs)) +\n  geom_point(alpha = 0.7, color = \"mediumorchid\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(\n    title = \"XGBoost — Observed vs Predicted Streamflow\",\n    x = \"Predicted q_mean\",\n    y = \"Observed q_mean\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "lab6.html#the-xgboost-model-worked-well-on-the-test-data-it-predicted-values-that-are-very-similar-to-the-observed-streamflow-values.-the-loglog-scatterplot-shows-a-cluster-of-points-around-the-11-line-which-shows-accurate-predictions.-there-is-not-a-lot-of-spread-even-at-the-low-and-high-extremes-which-means-the-model-generalizes-well-and-shows-key-hydrological-patterns.-this-supports-the-high-r²-score-shown-during-cross-validation-and-reinforces-that-xgboost-as-a-good-choice-for-modeling-streamflow-in-this-dataset.",
    "href": "lab6.html#the-xgboost-model-worked-well-on-the-test-data-it-predicted-values-that-are-very-similar-to-the-observed-streamflow-values.-the-loglog-scatterplot-shows-a-cluster-of-points-around-the-11-line-which-shows-accurate-predictions.-there-is-not-a-lot-of-spread-even-at-the-low-and-high-extremes-which-means-the-model-generalizes-well-and-shows-key-hydrological-patterns.-this-supports-the-high-r²-score-shown-during-cross-validation-and-reinforces-that-xgboost-as-a-good-choice-for-modeling-streamflow-in-this-dataset.",
    "title": "Lab 6",
    "section": "The XGBoost model worked well on the test data, it predicted values that are very similar to the observed streamflow values. The log–log scatterplot shows a cluster of points around the 1:1 line which shows accurate predictions. There is not a lot of spread, even at the low and high extremes which means the model generalizes well and shows key hydrological patterns. This supports the high R² score shown during cross-validation and reinforces that XGBoost as a good choice for modeling streamflow in this dataset.",
    "text": "The XGBoost model worked well on the test data, it predicted values that are very similar to the observed streamflow values. The log–log scatterplot shows a cluster of points around the 1:1 line which shows accurate predictions. There is not a lot of spread, even at the low and high extremes which means the model generalizes well and shows key hydrological patterns. This supports the high R² score shown during cross-validation and reinforces that XGBoost as a good choice for modeling streamflow in this dataset."
  }
]